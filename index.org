#+title: Predicting Crypto Returns with PyTorch
#+author: Matt Brigida
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup


* Import Libraries and Get Data

#+begin_src python :session py
import pandas as pd
import torch
from torch import nn
#+end_src

#+RESULTS:

  
#+begin_src python :session py :cache true
import cbpro
import time
public_client = cbpro.PublicClient()

eth = public_client.get_product_historic_rates('ETH-USD')
time.sleep(5)
btc = public_client.get_product_historic_rates('BTC-USD')
time.sleep(5)
atom = public_client.get_product_historic_rates('ATOM-USD')
time.sleep(5)
ada = public_client.get_product_historic_rates('ADA-USD')
time.sleep(5)
ltc = public_client.get_product_historic_rates('LTC-USD')
#+end_src

#+RESULTS:


#+begin_src python :session py :cache true
eth = pd.DataFrame(eth, columns=["time", "low", "high", "open", "close", "volume"])
btc = pd.DataFrame(btc, columns=["time", "low", "high", "open", "close", "volume"])
atom = pd.DataFrame(atom, columns=["time", "low", "high", "open", "close", "volume"])
ada = pd.DataFrame(ada, columns=["time", "low", "high", "open", "close", "volume"])
ltc = pd.DataFrame(ltc, columns=["time", "low", "high", "open", "close", "volume"])
#+end_src

#+RESULTS:

#+begin_src python :session py :cache true
ethd = eth[['time', 'close', 'volume']]
ethd.columns.values[1] = "eth_close"
ethd.columns.values[2] = "eth_volume"

btcd = btc[['time', 'close', 'volume']]
btcd.columns.values[1] = "btc_close"
btcd.columns.values[2] = "btc_volume"

atomd = atom[['time', 'close', 'volume']]
atomd.columns.values[1] = "atom_close"
atomd.columns.values[2] = "atom_volume"

adad = ada[['time', 'close', 'volume']]
adad.columns.values[1] = "ada_close"
adad.columns.values[2] = "ada_volume"

ltcd = ltc[['time', 'close', 'volume']]
ltcd.columns.values[1] = "ltc_close"
ltcd.columns.values[2] = "ltc_volume"

data = ethd.merge(btcd, on = "time")
data = data.merge(atomd, on = "time")
data = data.merge(adad, on = "time")
data = data.merge(ltcd, on = "time")
data.set_index('time', inplace=True)

data = data.pct_change()[1:]
data
#+end_src

#+RESULTS:
#+begin_example
            eth_close  eth_volume  btc_close  btc_volume  atom_close  atom_volume  ada_close  ada_volume  ltc_close  ltc_volume
time                                                                                                                           
1629038880   0.002878   -0.272912   0.003729   -0.566322    0.003229    -0.665810  -0.000190    9.202048   0.004482   -0.785232
1629038820   0.001432   -0.842006   0.001807   -0.442548    0.002562    -0.126154   0.001897   -0.445789   0.001283    0.601307
1629038760   0.000899    2.095115   0.000722   -0.299291    0.000917     1.854754   0.001183    0.083344   0.001281   -0.710490
1629038700  -0.000773    0.380236  -0.000009   -0.302589   -0.000458    -0.831329   0.000614    0.008234  -0.000111    2.066424
1629038640   0.002430   -0.004609   0.001218    1.689570    0.002096    -0.775137   0.001748   -0.333411   0.002114    3.065531
...               ...         ...        ...         ...         ...          ...        ...         ...        ...         ...
1629021480  -0.000752    0.624889  -0.000429   -0.707476   -0.000522    -0.675610  -0.000844   -0.414862  -0.000724   -0.536428
1629021420  -0.000275   -0.060528   0.000061    4.792804   -0.001241    -0.789474  -0.002299    1.375979   0.001059   -0.763898
1629021360   0.000126   -0.046568   0.000339    0.323860    0.001046     3.607143  -0.001129   -0.695961   0.000000    1.449336
1629021300   0.000711    0.022483  -0.000076   -0.353646    0.000131    -0.666667   0.000330    0.250849   0.000223   -0.671973
1629021240   0.000686    0.073074   0.000674    0.424199    0.001110    63.953488   0.001224    0.945896   0.000780   13.967535

[294 rows x 10 columns]
#+end_example

* Pytorch Neural Network

** Training vs Test Sets

#+begin_src python :session py :cache true
train = data[0:(len(data)-30)]
test = data[(len(data)-30): len(data)]
#+end_src

*** Lag Variables

First pop the targets, which are not lagged.

#+begin_src python :session py :cache true
train_target = train.pop('atom_close')
train_target = train_target[1:]
test_target = test.pop('atom_close')
test_target = test_target[1:]
#+end_src

Lag the RHS variables.

#+begin_src python :session py :cache true
train_features = train.shift(1)[1:]
test_features = test.shift(1)[1:]
#+end_src

#+begin_src python :session py :cache true
train_features = torch.tensor(train_features.values, device='cuda')
train_target = torch.tensor(train_target.values, device='cuda')
#+end_src

#+RESULTS:

#+begin_src python :session py :cache true
test_features = torch.tensor(test_features.values, device='cuda')
test_target = torch.tensor(test_target.values, device='cuda')
#+end_src

#+RESULTS:



** Build Model

*** More Simple Model

#+begin_src python :session py :cache true
model = torch.nn.Sequential(
    torch.nn.Linear(9, 10),
    torch.nn.Linear(10, 1)
)
#+end_src

#+RESULTS:



#+begin_src python :session py :cache true :exports both :results output :eval yes
device = 'cuda'
#model = AR_model().to(device)
model = model.to(device)
model
#+end_src

#+RESULTS:

*** Training

#+begin_src python :session py :cache true :exports both :results output
learning_rate = 1e-3

loss_fn = nn.MSELoss()
#+end_src

#+RESULTS:

#+begin_src python :session py :cache true :exports both :results output
for t in range(4000):

    # Forward pass: compute predicted y by passing x to the model. Module objects
    # override the __call__ operator so you can call them like functions. When
    # doing so you pass a Tensor of input data to the Module and it produces
    # a Tensor of output data.
    y_pred = model(train_features.float()).float()

    # Compute and print loss. We pass Tensors containing the predicted and true
    # values of y, and the loss function returns a Tensor containing the
    # loss.
    loss = loss_fn(y_pred, train_target.float())
    if t % 100 == 99:
        print(t, loss.item())

    # Zero the gradients before running the backward pass.
    model.zero_grad()

    # Backward pass: compute gradient of the loss with respect to all the learnable
    # parameters of the model. Internally, the parameters of each Module are stored
    # in Tensors with requires_grad=True, so this call will compute gradients for
    # all learnable parameters in the model.
    loss.backward()

    # Update the weights using gradient descent. Each parameter is a Tensor, so
    # we can access its gradients like we did before.
    with torch.no_grad():
        for param in model.parameters():
            param -= learning_rate * param.grad
#+end_src

#+RESULTS:
#+begin_example
99 0.05186983570456505
199 0.022924993187189102
299 0.011585462838411331
399 0.006629244424402714
499 0.004201109055429697
599 0.0028635328635573387
699 0.0020454032346606255
799 0.001503319013863802
899 0.0011241636238992214
999 0.000849790230859071
1099 0.0006470752414315939
1199 0.0004953877069056034
1299 0.0003809727495536208
1399 0.00029422054649330676
1499 0.00022820846061222255
1599 0.00017785072850529104
1699 0.00013936242612544447
1799 0.00010990101873176172
1899 8.732201240491122e-05
1999 6.99987358530052e-05
2099 5.669524398399517e-05
2199 4.64686490886379e-05
2299 3.860050128423609e-05
2399 3.2540840038564056e-05
2499 2.7869326004292816e-05
2599 2.4264318199129775e-05
2699 2.147868690371979e-05
2799 1.9323406377225183e-05
2899 1.7653757822699845e-05
2999 1.6357724234694615e-05
3099 1.53501005115686e-05
3199 1.4565261153620668e-05
3299 1.395232629874954e-05
3399 1.3472427781380247e-05
3499 1.3095865142531693e-05
3599 1.2799368960259017e-05
3699 1.25647702589049e-05
3799 1.2378705832816195e-05
3899 1.2230881111463532e-05
3999 1.2112318472645711e-05
#+end_example

*** Result

#+begin_src python :session py :cache true :exports both :results output
pred_test_target = model(test_features.float())
#print(pred_test_target)
test_loss = (test_target - pred_test_target).abs()
print(test_loss.sum())
#+end_src

#+RESULTS:
: tensor(3.4659, device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward0>)
