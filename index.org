#+title: Predicting Crypto Returns with PyTorch
#+author: Matt Brigida
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup


* Import Libraries and Get Data

#+begin_src python :session py
import pandas as pd
import torch
from torch import nn
import seaborn as sns
sns.set_theme(style="darkgrid")
import matplotlib.pyplot as plt
plt.style.use("dark_background")
#+end_src

#+RESULTS:
: None

  
#+begin_src python :session py :cache true
import cbpro
import time
public_client = cbpro.PublicClient()

eth = public_client.get_product_historic_rates('ETH-USD')
time.sleep(5)
btc = public_client.get_product_historic_rates('BTC-USD')
time.sleep(5)
atom = public_client.get_product_historic_rates('ATOM-USD')
time.sleep(5)
ada = public_client.get_product_historic_rates('ADA-USD')
time.sleep(5)
ltc = public_client.get_product_historic_rates('LTC-USD')
#+end_src

#+RESULTS:


#+begin_src python :session py :cache true
eth = pd.DataFrame(eth, columns=["time", "low", "high", "open", "close", "volume"])
btc = pd.DataFrame(btc, columns=["time", "low", "high", "open", "close", "volume"])
atom = pd.DataFrame(atom, columns=["time", "low", "high", "open", "close", "volume"])
ada = pd.DataFrame(ada, columns=["time", "low", "high", "open", "close", "volume"])
ltc = pd.DataFrame(ltc, columns=["time", "low", "high", "open", "close", "volume"])
#+end_src

#+RESULTS:

#+begin_src python :session py :cache true
ethd = eth[['time', 'close', 'volume']]
ethd.columns.values[1] = "eth_close"
ethd.columns.values[2] = "eth_volume"

btcd = btc[['time', 'close', 'volume']]
btcd.columns.values[1] = "btc_close"
btcd.columns.values[2] = "btc_volume"

atomd = atom[['time', 'close', 'volume']]
atomd.columns.values[1] = "atom_close"
atomd.columns.values[2] = "atom_volume"

adad = ada[['time', 'close', 'volume']]
adad.columns.values[1] = "ada_close"
adad.columns.values[2] = "ada_volume"

ltcd = ltc[['time', 'close', 'volume']]
ltcd.columns.values[1] = "ltc_close"
ltcd.columns.values[2] = "ltc_volume"

data = ethd.merge(btcd, on = "time")
data = data.merge(atomd, on = "time")
data = data.merge(adad, on = "time")
data = data.merge(ltcd, on = "time")
data.set_index('time', inplace=True)

data = data.pct_change()[1:]
data.to_csv("close_vol_data.csv")
data
#+end_src

#+RESULTS:
#+begin_example
            eth_close  eth_volume  btc_close  btc_volume  atom_close  atom_volume  ada_close  ada_volume  ltc_close  ltc_volume
time                                                                                                                           
1632577740  -0.000780    0.154219   0.000025    0.144183   -0.000540     0.592147  -0.003211    0.444062  -0.000593   -0.460877
1632577680   0.000188   -0.002027  -0.000169    0.459292   -0.000859     0.115593  -0.003221   -0.841104  -0.000857   -0.889853
1632577620  -0.000644    2.082848  -0.000444    0.991550   -0.001106     0.709477  -0.001091    1.484608   0.000066   24.193727
1632577560  -0.001131   -0.876900  -0.000117   -0.647560    0.000000     1.137118  -0.000294   -0.010421  -0.000791   -0.967124
1632577500   0.000738    1.264151   0.000308    0.581940    0.001673    -0.796686   0.001009   -0.341500   0.000198    4.348043
...               ...         ...        ...         ...         ...          ...        ...         ...        ...         ...
1632560400   0.003388    0.815562   0.003340   -0.107485    0.004578    -0.366925   0.004614    0.536060   0.004629   -0.541633
1632560340  -0.000790   -0.269769  -0.001497    1.789701   -0.004192    -0.218117  -0.004979   -0.027378  -0.005581    0.565607
1632560280   0.001279    0.910556   0.001156    1.717195    0.002447     0.678753   0.000992    0.340219   0.002349    0.128134
1632560220  -0.001538   -0.435504  -0.002279   -0.660321   -0.001880    -0.407981  -0.001379   -0.586200  -0.002669   -0.620945
1632560160  -0.001922    0.568542  -0.004131   -0.065882   -0.000538    -0.339289  -0.001424    0.563350  -0.003329   -0.146626

[294 rows x 10 columns]
#+end_example


** Training vs Test Sets

#+begin_src python :session py :cache true
train = data[0:(len(data)-30)]
test = data[(len(data)-30): len(data)]
#+end_src

#+RESULTS:

*** Lag Variables

First pop the targets, which are not lagged.

#+begin_src python :session py :cache true
train_target = train.pop('atom_close')
train_target = train_target[1:]
test_target = test.pop('atom_close')
test_target = test_target[1:]
#+end_src

#+RESULTS:

Lag the RHS variables.

#+begin_src python :session py :cache true
train_features = train.shift(1)[1:]
test_features = test.shift(1)[1:]
#+end_src

#+RESULTS:

* Viz

#+begin_src python :session py
eth_target_plot = sns.relplot(x=train_features["eth_close"], y=train_target)
eth_target_plot.savefig("eth_target_plot.png")
#+end_src

#+RESULTS:
: None

[[./eth_target_plot.png]]

* Pytorch Neural Network

#+begin_src python :session py :cache true
train_features = torch.tensor(train_features.values, device='cuda')
train_target = torch.tensor(train_target.values, device='cuda')
#+end_src

#+RESULTS:

#+begin_src python :session py :cache true
test_features = torch.tensor(test_features.values, device='cuda')
test_target = torch.tensor(test_target.values, device='cuda')
#+end_src

#+RESULTS:

** Build Model

*** More Simple Model

#+begin_src python :session py :cache true
model = torch.nn.Sequential(
    torch.nn.Linear(9, 10),
    torch.nn.Linear(10, 1)
)
#+end_src

#+RESULTS:



#+begin_src python :session py :cache true :exports both :results output :eval yes
device = 'cuda'
#model = AR_model().to(device)
model = model.to(device)
model
#+end_src

#+RESULTS:

*** Training

#+begin_src python :session py :cache true :exports both :results output
learning_rate = 1e-3

loss_fn = nn.MSELoss()
#+end_src

#+RESULTS:

#+begin_src python :session py :cache true :exports both :results output
for t in range(4000):

    y_pred = model(train_features.float()).float()

    loss = loss_fn(y_pred, train_target.float())
    if t % 100 == 99:
        print(t, loss.item())

    model.zero_grad()

    loss.backward()

    with torch.no_grad():
        for param in model.parameters():
            param -= learning_rate * param.grad
#+end_src

#+RESULTS:
#+begin_example
99 0.05186983570456505
199 0.022924993187189102
299 0.011585462838411331
399 0.006629244424402714
499 0.004201109055429697
599 0.0028635328635573387
699 0.0020454032346606255
799 0.001503319013863802
899 0.0011241636238992214
999 0.000849790230859071
1099 0.0006470752414315939
1199 0.0004953877069056034
1299 0.0003809727495536208
1399 0.00029422054649330676
1499 0.00022820846061222255
1599 0.00017785072850529104
1699 0.00013936242612544447
1799 0.00010990101873176172
1899 8.732201240491122e-05
1999 6.99987358530052e-05
2099 5.669524398399517e-05
2199 4.64686490886379e-05
2299 3.860050128423609e-05
2399 3.2540840038564056e-05
2499 2.7869326004292816e-05
2599 2.4264318199129775e-05
2699 2.147868690371979e-05
2799 1.9323406377225183e-05
2899 1.7653757822699845e-05
2999 1.6357724234694615e-05
3099 1.53501005115686e-05
3199 1.4565261153620668e-05
3299 1.395232629874954e-05
3399 1.3472427781380247e-05
3499 1.3095865142531693e-05
3599 1.2799368960259017e-05
3699 1.25647702589049e-05
3799 1.2378705832816195e-05
3899 1.2230881111463532e-05
3999 1.2112318472645711e-05
#+end_example

*** Result

#+begin_src python :session py :cache true :exports both :results output
pred_test_target = model(test_features.float())
#print(pred_test_target)
test_loss = (test_target - pred_test_target).abs()
print(f'The mean loss is: {test_loss.mean()}')
print(f'The standard deviation of the test returns is: {test_target.std()}')
#+end_src

#+RESULTS:
: The mean loss is: 0.008173128249004055
: The standdard deviation of the test returns is: 0.0011275225052620012

So the model is performing poorly.

* Pytorch Pyro

#+begin_comment
https://pyro.ai/examples/forecasting_i.html
#+end_comment

#+begin_src python :session pyro :cache true :exports both :results output
import torch
import pyro
import pyro.distributions as dist
import pyro.poutine as poutine
from pyro.contrib.examples.bart import load_bart_od
from pyro.contrib.forecast import ForecastingModel, Forecaster, backtest, eval_crps
from pyro.infer.reparam import LocScaleReparam, StableReparam
from pyro.ops.tensor_utils import periodic_cumsum, periodic_repeat, periodic_features
from pyro.ops.stats import quantile
import matplotlib.pyplot as plt
import pandas as pd
#+end_src

#+RESULTS:


** Training vs Test Sets (again new session)

#+begin_src python :session pyro :cache true
data = pd.read_csv("./close_vol_data.csv")
train = data[0:(len(data)-30)]
test = data[(len(data)-30): len(data)]
#+end_src

#+RESULTS:

*** Lag Variables

First pop the targets, which are not lagged.

#+begin_src python :session pyro :cache true
train_target = train.pop('atom_close')
train_target = train_target[1:]
test_target = test.pop('atom_close')
test_target = test_target[1:]
#+end_src

#+RESULTS:

Lag the RHS variables.

#+begin_src python :session pyro :cache true
train_features = train.shift(1)[1:]
test_features = test.shift(1)[1:]
#+end_src

#+RESULTS:

#+begin_src python :session pyro :cache true
train_features = torch.tensor(train_features.values, device='cuda')
train_target = torch.tensor(train_target.values, device='cuda')
#+end_src

#+RESULTS:

#+begin_src python :session pyro :cache true
test_features = torch.tensor(test_features.values, device='cuda')
test_target = torch.tensor(test_target.values, device='cuda')
#+end_src

#+RESULTS:


#+begin_src python :session pyro :cache true
class Model1(ForecastingModel):
    def model(self, zero_data, covariates):
        data_dim = zero_data.size(-1)
        feature_dim = covariates

        bias = pyro.sample("bias", dist.Normal(0, 10).expand([data_dim]).to_event(1))
        weight = pyro.sample("weight", dist.Normal(0, 0.1).expand([feature_dim]).to_event(1))
        prediction = bias + (weight * covariates).sum(-1, keepdim=True)

        assert prediction.shape[-2:] == zero_data.shape

        noise_scale = pyro.sample("noise_scale", dist.LogNormal(-5, 5).expand([1]).to_event(1))
        noise_dist = dist.Normal(0, noise_scale)

        self.predict(noise_dist, prediction)
#+end_src

#+RESULTS:

#+begin_src python :session pyro :cache true
forecaster = Forecaster(Model1(), train_target, train_features, learning_rate=0.1)
#+end_src

#+RESULTS:

* Tensorflow




* Linear Vector-Autoregression

We'll use R:


#+begin_src R :session *R* :exports both :results output
library(vars)
data <- read.csv("./close_vol_data.csv")
data <- data[,-1]
sd(VAR(data)$varresult$atom_close$residuals)
#+end_src

#+RESULTS:
: [1] 0.002377701

Slightly larger than the standard deviation of the test returns, but lower than PyTorch.  Of course this formulation uses more than just lags.
